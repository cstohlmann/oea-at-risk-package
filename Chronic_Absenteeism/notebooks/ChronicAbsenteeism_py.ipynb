{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Chronic Absenteeism Package Class Notebook\r\n",
        "\r\n",
        "Currently contained/provided within this notebook: \r\n",
        " - Method for reading and writing to stage 2p, and then also writing to stage 3p (goal for package: separate methods for p vs. np processing).\r\n",
        " - Activity Data Schema Mapping (as defined for Insights data at this point).\r\n",
        "\r\n",
        " Creates 1 new table (based on IMS Globals Caliper's standards with a few slight changes): Event.\r\n",
        " Below describes the schema/column mappings from Activity tables to the new, OEA standardized tables:\r\n",
        "1. Event:\r\n",
        "    - __id:__ unique ID used as a signal key *(e.g. Insights Activity Table: SignalId)*\r\n",
        "    - __type:__ type of Activity signal *(e.g. Insights Activity Table: AppName)*\r\n",
        "    - __actor:__ student or teacher that created the signal *(e.g. Insights Activity Table: ActorId)*\r\n",
        "    - __ipAddress:__ IP address origination of signal *(e.g. Graph SignInAuditLogs Activity Table: ipAddress)*\r\n",
        "    - __action:__ description of signal (or signal type) *(e.g. Insights Activity Table: SignalType)*\r\n",
        "    - __object:__ mapping to SoftwareApplication table for activity signal data source *(e.g. \"Insights\")*\r\n",
        "    - __eventTime:__ date/timestamp of the activity signal *(e.g. Insights Activity Table: StartTime)*\r\n",
        "    - __details:__ measures/details on some activities *(e.g. Insights Activity Table: MeetingDuration if the SignalType is Meeting)*\r\n",
        "    - __version:__ current schema version being processed *(e.g. Insights Activity Table: SchemaVersion)*\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Unsure if this is the best route\r\n",
        "class ChronicAbsenteeism(BaseOEAModule):\r\n",
        "    \"\"\"\r\n",
        "    Currently, package class notebook only contains processing for stage2p data.\r\n",
        "     - Reads activity data from stage2p, writes the activity data schema/relationship mapping to stage2p again\r\n",
        "     - Then takes this mapping and generalized and method for writing to stage3p\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, source_folder='ActivitySchema'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "        print(self.stage2p)\r\n",
        "        \r\n",
        "        self.stage2p_insightsActivity = oea.path('stage2p', directory_path='M365')\r\n",
        "        print(self.stage2p_insightsActivity)\r\n",
        "\r\n",
        "        # work on partitioning and model history versioning place\r\n",
        "        self.schemas['ActivityEvents'] = [['id', 'string', 'no-op'],\r\n",
        "                        ['type', 'string', 'no-op'],\r\n",
        "                        ['actor', 'string', 'no-op'],\r\n",
        "                        ['action', 'string', 'no-op'],\r\n",
        "                        ['object', 'string', 'no-op'],\r\n",
        "                        ['eventTime', 'timestamp', 'no-op'],\r\n",
        "                        ['details', 'string', 'no-op'], \r\n",
        "                        ['version', 'string', 'no-op']]\r\n",
        "\r\n",
        "        self.schemas['TechActivity'] = [['SignalType', 'string', 'no-op'],\r\n",
        "                        ['StartTime', 'string', 'no-op'],\r\n",
        "                        ['UserAgent', 'string', 'no-op'],\r\n",
        "                        ['SignalId', 'string', 'hash'],\r\n",
        "                        ['SisClassId', 'string', 'hash'],\r\n",
        "                        ['ClassId', 'string', 'hash'],\r\n",
        "                        ['ChannelId', 'string', 'hash'],\r\n",
        "                        ['AppName', 'string', 'no-op'],\r\n",
        "                        ['ActorId', 'string', 'hash'],\r\n",
        "                        ['ActorRole', 'string', 'no-op'],\r\n",
        "                        ['SchemaVersion', 'string', 'no-op'],\r\n",
        "                        ['AssignmentId', 'string', 'hash'],\r\n",
        "                        ['SubmissionId', 'string', 'hash'],\r\n",
        "                        ['SubmissionCreatedTime', 'timestamp', 'no-op'],\r\n",
        "                        ['Action', 'string', 'no-op'],\r\n",
        "                        ['DueDate', 'string', 'no-op'],\r\n",
        "                        ['ClassCreationDate', 'string', 'no-op'],\r\n",
        "                        ['Grade', 'string', 'no-op'],\r\n",
        "                        ['SourceFileExtension', 'string', 'no-op'],\r\n",
        "                        ['MeetingDuration', 'string', 'no-op'],\r\n",
        "                        ['MeetingSessionId', 'string', 'hash'],\r\n",
        "                        ['MeetingType', 'string', 'no-op']]\r\n",
        "    \r\n",
        "\r\n",
        "    # might be able to remove this parent method??                        \r\n",
        "    def curate_activityStage2p(self):\r\n",
        "        \"\"\"  Processes insights data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
        "        logger.info(\"Processing microsoft_insights data from: \" + self.stage2p_insightsActivity)\r\n",
        "\r\n",
        "        # need a more standardized approach for how to process specified-date data\r\n",
        "        items = mssparkutils.fs.ls(self.stage2p_insightsActivity)\r\n",
        "        for item in items:\r\n",
        "            if item.name == \"year=2021\":\r\n",
        "                logger.info(\"Change the year hardcoded for specific year data if you wish to process more than just 2021 data\")\r\n",
        "                self._create_oea_standard_activity_table()\r\n",
        "                #self._process_activity_stage2p_data()\r\n",
        "            elif item.name == \"_checkpoints_p\":\r\n",
        "                logger.info(\"Ignore processing of the checkpoints from previous curation\")\r\n",
        "            elif item.name == \"_delta_log\":\r\n",
        "                logger.info(\"Ignore processing the delta log from the previous module ingestion\")\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for curating this data\")\r\n",
        "                \r\n",
        "        logger.info(\"Finished curating stage 2 activity data\")\r\n",
        "\r\n",
        "    def _create_oea_standard_activity_table(self):\r\n",
        "        \"\"\" Processes Insights activity data from stage2p and creates the OEA standard activity table, writing back to stage2p. \"\"\"\r\n",
        "        logger.info(\"Processing activity data from: \" + self.stage2p_insightsActivity)\r\n",
        "      \r\n",
        "        # currently hardcoding in year=2021 for testing purposes\r\n",
        "        dfActivity = oea.load_delta('stage2p/M365/TechActivity_pseudo/year=2021')\r\n",
        "\r\n",
        "        \"\"\" method for creating Activity Data \"Event\" table for schema mapping \"\"\"\r\n",
        "        df = dfActivity.select(['SignalId','ActorId_pseudonym','SignalType','StartTime','MeetingDuration','SchemaVersion','AppName'])\r\n",
        "        df = df.withColumnRenamed(\"SignalId\", \"id\")\\\r\n",
        "                    .withColumnRenamed(\"ActorId_pseudonym\", \"actor\")\\\r\n",
        "                    .withColumnRenamed(\"SignalType\", \"action\")\\\r\n",
        "                    .withColumnRenamed(\"StartTime\", \"eventTime\")\\\r\n",
        "                    .withColumnRenamed(\"MeetingDuration\", \"details\")\\\r\n",
        "                    .withColumnRenamed(\"SchemaVersion\", \"version\")\\\r\n",
        "                    .withColumnRenamed(\"AppName\", \"type\")\r\n",
        "        \r\n",
        "        df = df.withColumn('object', F.lit('MSInsights'))\r\n",
        "\r\n",
        "        df = df.withColumn('year', F.year(F.col('eventTime'))).withColumn('month', F.month(F.col('eventTime')))\r\n",
        "\r\n",
        "        df.write.save(self.stage2p, format='delta', mode='overwrite', partitionBy=['year', 'month'], overwriteSchema='true')\r\n",
        "\r\n",
        "    def _process_activity_stage2p_data(self):\r\n",
        "        \"\"\" Processes OEA standard activity table from stage2 into stage3 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing activity data from: \" + self.stage2p + '/_OEA')\r\n",
        "        activity_spark_schema = oea.to_spark_schema(self.schemas['ActivityEvents'])\r\n",
        "        dfActivity = spark.readStream.format('parquet').load(self.stage2p + '/_OEA_ActivityEvents/', header='true', schema=activity_spark_schema)\r\n",
        "\r\n",
        "        # Writing out ActivityEvents table to stage 3p\r\n",
        "        if len(dfEvent_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage3p')\r\n",
        "        else:\r\n",
        "            query = dfEvent_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage2p + '_OEA_ActivityEvents/_checkpoints_p').partitionBy('curationYearMonth')\r\n",
        "            query = query.start(self.stage3p + '/ActivityEvents_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "            logger.info(query.lastProgress)\r\n",
        "  \r\n",
        ""
      ]
    }
  ]
}